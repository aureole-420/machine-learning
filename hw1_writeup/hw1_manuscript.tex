\documentclass[english,11pt]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption} % to plot subfigure
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\geometry{verbose,tmargin=2.5cm,bmargin=2.5cm,lmargin=2.5cm,rmargin=2.5cm}
\usepackage{amsmath}
\usepackage{babel}
\begin{document}



\title{COMP 540 HW 01 }

\author{Lyu Pan (lp28), Yuhui Tong (yt30)}
\maketitle



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% part 0 %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\part*{0. Background Refresher}

\subsection{Samplers} 

\begin{figure}[h]
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.8\linewidth]{samplefigs/fig1.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{samplefigs/fig2.png}
\end{subfigure}

\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.8\linewidth]{samplefigs/fig3.png}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{samplefigs/fig4.png}
\end{subfigure}
\caption{Visualization of four distributions}
\label{fig:samplers}
\end{figure}

Figure~\ref{fig:samplers} visualize four distributions. Specifically, for a mixture distribution of four 2D Gaussians, the probability that a sample from this distribution lies within the unit circle centered at $(0.1, 0.2)$ is $p = 0.83736 $.


\subsection{Prove two independent Poisson random variables are also Poisson variable.}

Proof:

Given two independent random variables $X_{1}\sim P(\lambda_{1})$
and $X\sim P(\lambda_{2})$. i.e.,

\begin{equation}
P(X_{1}=m)=e^{-\lambda_{1}}\frac{\lambda_{1}^{m}}{m!},\ m=1,2,...    
\end{equation}


and

\begin{equation}
P(X_{2}=n)=e^{-\lambda_{2}}\frac{\lambda_{2}^{n}}{n!},\ n=0,1,2,...    
\end{equation}


Denote sum of them are 
\begin{equation}
X=X_{1}+X_{2}
\end{equation}

then the probability distribution of $X$ is:

\begin{eqnarray}
P(X=k) & = & \sum_{m+n=k}P(X_{1}=m,X_{2}=n) \notag \\
 & \overset{X_{1},X_{2}indep.R.V.}{=} & \sum_{m+n=k}P(X_{1}=m)P(X_{2}=n) \notag \\
 & = & \sum_{m+n=k}e^{-\lambda_{1}}\frac{\lambda_{1}^{m}}{m!}e^{-\lambda_{2}}\frac{\lambda_{2}^{n}}{n!} \notag \\
 & = & \frac{1}{k!}e^{-(\lambda_{1}+\lambda_{2})}\sum_{m+n=k}\frac{k!}{m!n!}\lambda_{1}^{m}\lambda_{2}^{n} \notag \\
 & = & \frac{1}{k!}e^{-(\lambda_{1}+\lambda_{2})}(\lambda_{1}+\lambda_{2})^{k}
\end{eqnarray}

that is, $X=X_{1}+X_{2}\sim P(\lambda_{_{1}}+\lambda_{2})$. Q.E.D.

\subsection{Proof question 3}

\begin{eqnarray}
p(x_{1,}x_{0}) & = & p(x_{1}|x_{0})p(x_{0}) \notag \\
 & = & \alpha e^{-\frac{(x_{1}-x_{0})^{2}}{2\sigma^{2}}}\alpha_{0}e^{-\frac{(x_{0}-\mu_{0})^{2}}{2\sigma_{0}^{2}}}
\end{eqnarray}

The probability of $X_{1} = x_{1}$ is,
\begin{align*}
p\left ( X_{1}=x_{1} \right )&=\int p\left ( X_{1}=x_{1},X_{0}=x_{0} \right )dx_{0}
\\ &=\alpha _{0}\alpha \int exp\left ( -\frac{1}{2}\left ( \frac{\left ( x_{0}-\mu _{0} \right )^{2}}{\sigma ^{2}_{0}}+ \frac{\left ( x_{1}-x_{0} \right )^{2}}{\sigma^{2} }\right ) \right )dx_{0}
\\ &=\frac{\alpha _{0}\alpha }{A}exp\left ( -\frac{1}{2}\frac{\left ( x_{1}-\mu _{0} \right )^{2}}{\sigma^{2}+\sigma _{0}^{2}} \right )
\end{align*}
Here $A$ is a constant. So 
\begin{align*}
p\left ( X_{1}=x_{1} \right )=\alpha _{1}exp\left ( -\frac{1}{2}\frac{\left ( x_{1}-\mu _{1} \right )^{2}}{\sigma _{1}^{2}} \right )
\end{align*}
And
\begin{equation*}
\mu _{1}=\mu _{0}
\end{equation*}
\begin{equation*}
\sigma _{1}^{2}=\sigma^{2}+\sigma _{0}^{2}
\end{equation*}
\begin{equation*}
\alpha _{1}=\frac{\alpha _{0}\alpha }{A}=\alpha _{0}\alpha\int exp\left (-\frac{1}{2} \frac{x_{0}^{2}-2\left ( \frac{\sigma ^{2}\mu _{0}+\sigma _{0}^{2}x_{1}}{\sigma ^{2}+\sigma _{0}^{2}} \right )x_{0}+\left ( \frac{\sigma ^{2}\mu _{0}+\sigma _{0}^{2}x_{1}}{\sigma ^{2}+\sigma _{0}^{2}} \right )^{2}}{\sigma _{0}^{2}\sigma^{2} /\left ( \sigma ^{2}+\sigma _{0}^{2} \right )} \right )dx_{0}
\end{equation*}

\subsection{Eigenvalues}

\[
\boldsymbol{A}=\left[\begin{array}{cc}
13 & 5\\
2 & 4
\end{array}\right]
\]

Solving the eigen equations:
\begin{eqnarray*}
\boldsymbol{A}\boldsymbol{X} & = & \lambda\boldsymbol{X}\\
(\boldsymbol{A}-\lambda\boldsymbol{I})\boldsymbol{X} & = & 0\\
\left[\begin{array}{cc}
13-\lambda & 5\\
2 & 4-\lambda
\end{array}\right]\boldsymbol{X} & = & 0
\end{eqnarray*}
gives the eigen value and eigen vectors (not normalized):

$\lambda_{1}=3$, $\boldsymbol{X}_{1}=\left(\begin{array}{c}
-1/2\\
1
\end{array}\right)$

$\lambda_{2}=14$, $\boldsymbol{X}_{2}=\left(\begin{array}{c}
5\\
1
\end{array}\right)$

\subsection{Matrix multiplication}

$A=\left[\begin{array}{cc}
1 & 2\\
3 & 4
\end{array}\right]$, $B=\left[\begin{array}{cc}
5 & 6\\
7 & 8
\end{array}\right]$, we have $(A+B)^{2}\neq A^{2}+2AB+B^{2}$ 

$A=\left[\begin{array}{cc}
0 & 1\\
0 & 1
\end{array}\right]$, $B=\left[\begin{array}{cc}
1 & 1\\
0 & 0
\end{array}\right]$ , $A,B\neq0$ and we have $AB=0$

\subsection{Question 6}

\begin{eqnarray}
A^{T}A & = & (I-2uu^{T})^{T}(I-2uu^{T}) \notag \\
 & = & (I-2uu^{T})(I-2uu^{T}) \notag \\
 & = & I-2uu^{T}-2uu^{T}+4u(u^{T}u)u^{T} \notag \\
 & = & I
\end{eqnarray}

\subsection{Convex function}
\subsubsection{Triple}
for $x\geq 0$
\begin{equation}
f(x)  =  3x^{3}
\end{equation}
\begin{equation}
f^{'}(x)  =  3x^{2} 
\end{equation}
\begin{equation}
f^{''}(x)  =  6x \geq 0
\end{equation}
so,$f(x)=x^{3}$ is convex.
\subsubsection{Two dimension}
For any $ \lambda\in[0,1] $,\\
we have $ \lambda \geq 0,  1 - \lambda \geq 0$ \\
For any $(x_{1}, y_{1}), (x_{2}, y_{2}) on R^{2}$
\begin{eqnarray}
f(\lambda(x_{1}, y_{1}) + (1-\lambda)(x_{2}, y_{2})) & = & max(\lambda(x_{1}, y_{1}) + (1-\lambda)(x_{2}, y_{2})) \notag \\
& \leq & max(\lambda(x_{1}, y_{1})) + max((1-\lambda)(x_{2}, y_{2})) \notag \\
& = & \lambda max(x_{1},y_{1}) + (1- \lambda)max(x_{2}, y_{2}) \notag \\
& = & \lambda f(x_{1}, y_{1}) + (1-\lambda)f(x_{2}, y_{2}) 
\end{eqnarray}
So, $f(x)$ is convex.
\subsubsection{Plus}
Because $f$ is convex on $S$, for $\lambda \in [0,1]$ and all $x_{1}, x_{2} \in S$, we have \\
\begin{equation}
f(\lambda x_{1} + (1-\lambda) x_{2}) \leq \lambda f(x_{1}) + (1 - \lambda)f(x_{2}) 
\end{equation}
Also, \\ 
\begin{equation}
g(\lambda x_{1} + (1-\lambda) x_{2}) \leq \lambda g(x_{1}) + (1 - \lambda)g(x_{2})
\end{equation}
Let $h = f + g$,
\begin{eqnarray}
h(\lambda x_{1} + (1-\lambda) x_{2}) & = & f(\lambda x_{1} + (1-\lambda) x_{2}) +  g(\lambda x_{1} + (1-\lambda) x_{2}) \notag \\
& \leq & \lambda f(x_{1}) + (1 - \lambda)f(x_{2}) + \lambda g(x_{1}) + (1 - \lambda)g(x_{2}) \notag \\
& = & \lambda (f(x_{1}) + g(x_{1})) + (1 - \lambda)(f(x_{2}) + g(x_{2}))\notag \\
& = & \lambda h(x_{1}) + (1-\lambda)h(x_{2})
\end{eqnarray}
So, $h$ is convex, i.e. $f+g$ is convex.
\subsubsection{Multiply}
Let $h = fg$
\begin{eqnarray}
h^{''} & = & (f^{'}g + fg^{'})^{'}  \notag \\
& = & f^{''} + 2f^{'}g^{'} + fg^{''} 
\end{eqnarray}
Obviously, $f \geq 0$, $f^{''} \geq 0$, $g \geq 0$, $g^{''} \geq 0$ on $S$.\\
Let the minimum of both $f$ and $g$ be $x_{0}$, then when $x \leq x_{0}$,
$f(x_{0}) \leq 0$ and $g(x_{0}) \leq 0$; 
when $x \geq x_{0}$, $f(x_{0}) \geq 0$ and $g(x_{0}) \geq 0$, i.e. $f^{'}g^{'} \geq 0$. So 
\begin{eqnarray}
h^{''} \geq 0
\end{eqnarray} 
$h$ is convex, i.e. $f + g$ is convex.
\subsection{Entropy of categorical distribution}

The entropy of a categorical distribution on $K$ values is 

\begin{eqnarray}
H(p) & = & -\sum_{i=1}^{K}p_{i}\log(p_{i}),
\end{eqnarray}
with constraint that 
\begin{equation}
\sum_{i=1}^{K}p_{i}=1.    
\end{equation}


Using Lagrange Multiplier, one can combine the above two equations
into:

\begin{equation}
L(p,\lambda)=-\sum_{i=1}^{K}p_{i}\log(p_{i})+\lambda(\sum_{i=1}^{K}p_{i}-1).    
\end{equation}


Taking derivative of all unknown variables gives:

\begin{equation}
\frac{\partial L}{\partial p_{i}}=-(\log p_{i}+1)+\lambda=0,\ i=1,2,...    
\end{equation}


Substituting $p_{i}$ with $\lambda$ yields


\begin{equation}
p_{i}=\frac{1}{K},\ i=1,2,...,K.    
\end{equation}

Q.E.D.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% part 1 %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\part*{1. Locally weighted linear regression.}

\subsection*{1.1 Expression of $J(\theta)$}

Define $\boldsymbol{X},$ $\boldsymbol{W}$ in the following way:


\begin{equation}
\boldsymbol{X}=\left[\begin{array}{c}
----\left(x^{(1)}\right)^{T}----\\
----\left(x^{(2)}\right)^{T}----\\
\vdots\\
----\left(x^{(m)}\right)^{T}----
\end{array}\right]\Longleftrightarrow\boldsymbol{X}_{i,j}=(x^{(i)})_{j}    
\end{equation}

 
\begin{equation}
\boldsymbol{W}=\left[\begin{array}{cccc}
w^{(1)}\\
 & w^{(2)}\\
 &  & \ddots\\
 &  &  & w^{(m)}
\end{array}\right]\Longleftrightarrow\boldsymbol{W}_{i,j}=\delta_{i,j}w^{(i)}    
\end{equation}


Substituting $\boldsymbol{X},$ $\boldsymbol{W}$ into $(\boldsymbol{X}\theta-\boldsymbol{y})^{T}\boldsymbol{W}(\boldsymbol{X}\theta-\boldsymbol{y})$
yields

\begin{eqnarray*}
(\boldsymbol{X}\theta-\boldsymbol{y})^{T}\boldsymbol{W}(\boldsymbol{X}\theta-\boldsymbol{y}) & = & \sum_{i,j}\left[(\boldsymbol{X}\theta-\boldsymbol{y})^{T}\right]_{1,i}\boldsymbol{W}_{i,j}(\boldsymbol{X}\theta-\boldsymbol{y})_{j,1}\\
 & = & \sum_{i,j}\delta_{i,j}w^{(i)}\left[(\boldsymbol{X}\theta-\boldsymbol{y})^{T}\right]_{1,i}(\boldsymbol{X}\theta-\boldsymbol{y})_{j,1}\\
 & = & \sum_{i}w^{(i)}\left[(\boldsymbol{X}\theta-\boldsymbol{y})_{i,1}\right]^{2}\\
 & = & \sum_{i}w^{(i)}\left[\left(x^{(i)}\right)^{T}\theta-y^{(i)}\right]^{2}\\
 & = & \sum_{i}w^{(i)}\left[\theta x^{(i)}-y^{(i)}\right]^{2}.
\end{eqnarray*}
So $J(\theta)=\frac{1}{2}(\boldsymbol{X}\theta-\boldsymbol{y})^{T}\boldsymbol{W}(\boldsymbol{X}\theta-\boldsymbol{y})$.
Q.E.D.

\subsection*{1.2 Closed formed solution of $\theta$}

\begin{eqnarray*}
\frac{\partial J(\theta)}{\partial\theta} & = & \frac{1}{2}\frac{\partial}{\partial\theta}\left[(\boldsymbol{X}\theta-\boldsymbol{y})^{T}\boldsymbol{W}(\boldsymbol{X}\theta-\boldsymbol{y})\right]\\
 & = & \frac{1}{2}\frac{\partial}{\partial\theta}tr\left[\boldsymbol{y}^{T}\boldsymbol{W}\boldsymbol{X}\boldsymbol{\theta}-\boldsymbol{y}^{T}\boldsymbol{W}\boldsymbol{y}-\boldsymbol{\theta}^{T}\boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\theta}^{T}\boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{y}\right]\\
 & = & \boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{y}-\boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{X}\boldsymbol{\theta}
\end{eqnarray*}
where in the third line we used the following two equations\footnote{Take the second equation for example: 
\begin{eqnarray*}
\left[\frac{\partial tr(\boldsymbol{A}^{T}\boldsymbol{B}\boldsymbol{A})}{\partial\boldsymbol{A}}\right]_{i,j} & = & \frac{\partial A_{l,m}^{T}B_{m,n}A_{n,l}}{\partial A_{i,j}}=\frac{\partial A_{m,l}B_{m,n}A_{n,l}}{\partial A_{i,j}}\\
 & = & \delta_{m,i}\delta_{l,j}B_{m,n}A_{n,l}+A_{m,l}B_{m,n}\delta_{n,i}\delta_{l,j}\\
 & = & B_{i,n}A_{n,j}+A_{m,j}B_{m,i}\\
 & = & (\boldsymbol{B}\boldsymbol{A})_{i,j}+(\boldsymbol{B}^{T}\boldsymbol{A})_{i,j},
\end{eqnarray*}
hence leading to $\frac{\partial tr(\boldsymbol{A}^{T}\boldsymbol{B}\boldsymbol{A})}{\partial\boldsymbol{A}}=\boldsymbol{B}\boldsymbol{A}+\boldsymbol{B}^{T}\boldsymbol{A}$}:

\begin{equation}
\frac{\partial tr(\boldsymbol{A}^{T}\boldsymbol{B})}{\partial\boldsymbol{A}}=\boldsymbol{B}^{T}
\end{equation}

and 

\begin{equation}
\frac{\partial tr(\boldsymbol{A}^{T}\boldsymbol{B}\boldsymbol{A})}{\partial\boldsymbol{A}}=\boldsymbol{B}\boldsymbol{A}+\boldsymbol{B}^{T}\boldsymbol{A}    
\end{equation}

Therefore the closed form solution for $\theta$ is

\begin{equation}
\theta=(\boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\boldsymbol{W}\boldsymbol{y}.    
\end{equation}


\subsection*{1.3 Batch gradient descent for locally weighted linear regression}

The derivative of $J_{\theta}$ is:
\begin{equation}
\frac{\partial}{\partial\theta}J(\theta)=\sum_{i=1}^{m}w^{(i)}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}.     
\end{equation}
Therefore we have the following algorithm:

\begin{algorithm}[H]
\SetAlgoLined
\KwResult{The estimated parameters $\theta$ for locally weighted linear regression}
 \While{$\theta$ does not converge}{
    \For{every $j$} {
    $\theta_j = \theta_j -\alpha \sum_{i=1}^{m}w^{(i)}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}$\;}
}
 \caption{Batch gradient descent algorithm for locally weighted linear regression}
\end{algorithm}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%% part 2 %%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\part*{2. Properties of linear regression estimator.}

\subsection*{2.1 Prove $E[\theta]=\theta^{*}$}

Proof: 

The following facts:
\begin{enumerate}
\item $y^{(i)}=\theta^{*T}x^{(i)}+\epsilon^{(i)}$ ,
\item $\epsilon^{(i)},i=1,2,...,m$ are \emph{i.i.d.} of $N(0,\sigma^{2})$,
\end{enumerate}
indicate that:

given fixed arbitrary $x^{(i)}$ and fixed unknown parameter $\theta^{*}$,
$y^{(i)},\ 1\le i\le m$ are \emph{i.i.d. of $N(\theta^{*T}x^{(i)},\sigma^{2})$} 

\begin{equation}
p(y^{(i)}|x^{(i)},\theta^{*})=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y^{(i)}-\theta^{*T}x^{(i)})^{2}}{2\sigma^{2}}}.    
\end{equation}


The least-square estimate of $\theta^{*}$ is $\theta$ given by

\begin{eqnarray}
\theta & = & (\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\\
 & \overset{denote:(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\equiv\boldsymbol{A}}{=} & \boldsymbol{A}\boldsymbol{y}
\end{eqnarray}

The expectation of $\theta$ is

\begin{eqnarray}
E[\theta] & = & E[\boldsymbol{A}\boldsymbol{y}]\\
 & = & E\left[\begin{array}{c}
\sum_{j}A_{1,j}y^{(j)}\\
\sum_{j}A_{2,j}y^{(j)}\\
\vdots\\
\sum_{j}A_{d+1,j}y^{(j)}
\end{array}\right].
\end{eqnarray}

Since expectations has the following property (regardless of the independence
of $Z_{k}$):

\begin{equation}
E[Z_{1}+Z_{2}+...+Z_{l}]=\sum_{k}Z_{k},    
\end{equation}

we have 
\begin{eqnarray}
E[\theta] & = & E\left[\begin{array}{c}
\sum_{j}A_{1,j}y^{(j)}\\
\sum_{j}A_{2,j}y^{(j)}\\
\vdots\\
\sum_{j}A_{d+1,j}y^{(j)}
\end{array}\right]=\left[\begin{array}{c}
\sum_{j}A_{1,j}E[y^{(j)}]\\
\sum_{j}A_{2,j}E[y^{(j)}]\\
\vdots\\
\sum_{j}A_{d+1,j}E[y^{(j)}]
\end{array}\right] \\
 & = & \boldsymbol{A}\left[\begin{array}{c}
E[y^{(1)}]\\
E[y^{(2)}]\\
\vdots\\
E[y^{(m)}]
\end{array}\right]=\boldsymbol{A}\left[\begin{array}{c}
\theta^{*T}x^{(1)}\\
\theta^{*T}x^{(2)}\\
\vdots\\
\theta^{*T}x^{(m)}
\end{array}\right]\\
 & = & \boldsymbol{A}\boldsymbol{X}\theta^{*}=(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}\boldsymbol{X}\theta^{*}\\
 & = & \theta^{*}
\end{eqnarray}
where in the second step the following equation is used: $E[y^{(i)}]=\theta^{*T}x^{(i)}$
(trivial to obtain as $y^{(i)}$ observe normal distribution). Therefore
$E[\theta]=\theta^{*}$, implying that the estimation $\theta$ is
unbiased. Q.E.D

\subsection*{2.2 Prove $Var(\theta)=\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\sigma^{2}$}

Denote $\boldsymbol{A}=(\boldsymbol{X}^{T}\boldsymbol{X})^{-1}\boldsymbol{X}^{T}$
, therefore $\theta=\boldsymbol{A}\boldsymbol{y}$.

\begin{eqnarray}
Var(\theta) & = & Var(\boldsymbol{A}\boldsymbol{y})\\
 & = & Var(\left[\begin{array}{c}
\sum_{j}A_{1,j}y^{(j)}\\
\sum_{j}A_{2,j}y^{(j)}\\
\vdots\\
\sum_{j}A_{m,j}y^{(j)}
\end{array}\right])=\left[\begin{array}{c}
\sum_{j}A_{1,j}Var(y^{(j)})\\
\sum_{j}A_{2,j}Var(y^{(j)})\\
\vdots\\
\sum_{j}A_{m,j}Var(y^{(j)})
\end{array}\right]=\boldsymbol{A}\left[\begin{array}{c}
Var(y^{(1)})\\
Var(y^{(2)})\\
\vdots\\
Var(y^{(m)})
\end{array}\right],
\end{eqnarray}
where in the second line we made used of the property of variance:

\begin{eqnarray}
Var(\sum_{k}Z_{k}) & = & \sum Var(Z_{k}),\ Z_{k}\ is\ independent\ from\ each\ other.
\end{eqnarray}

Substituting $Var(y^{(i)})=\sigma^{2}\ i=1,2,...,m$, we have $Var(\theta)=\boldsymbol{A}\sigma^{2}=\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\sigma^{2}$.
Q.E.D.


\part*{3. Implementing linear regression and regularized linear regression}

\subsection*{3.1.A1}
No plot for this question.

\subsection*{3.1.A2}
Figure~\ref{fig:linear_fit} plots the linear fit using parameter obtained through training, while Fig.~\ref{fig:linear_convergence} shows the convergence of the loss function against iteration times.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{../hw1/part1/3_1_A_Linear.png}
\caption{Fitting a linear model to the data}
\label{fig:linear_fit}
\end{figure}


Figure~\ref{fig:linear_fit} plots the linear fit using parameter obtained through training.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{../hw1/part1/3_1_A_Convergence.png}
\caption{Convergence of Loss function against the number of iterations.}
\label{fig:linear_convergence}
\end{figure}

\subsection*{Problem 3.1.A3: Predicting on unseen data}
For lower status percentage = 5, we predict a median home value of 298034.494122

For lower status percentage = 50, we predict a median home value of -129482.128898

\subsection*{3.1.B1: Feature Normalization}
No plot for this question.

\subsection*{3.1.B2: Loss function and gradient descent}
Figure~\ref{fig:3_1_B2} shows the loss function against the number of iterations.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{../hw1/part1/fig3_1_B_Muiti.png}
\caption{Fitting a linear model to the data}
\label{fig:3_1_B2}
\end{figure}

\subsection*{3.1.B3 Making predictions on unseen data\label{subsec:3_1_B3}}
For average home in Boston suburbs, we predict a median home value of 225328.063241

\subsection*{3.1.B4: Normal equations}
For average home in Boston suburbs, we predict a median home value of 225328.063241, which is the same as we obtained in subsec.~3.1.B3.


\subsection*{Problem 3.1.B5: Exploring convergence of gradient descent}

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.8\linewidth]{../hw1/part1/3_1_B5_lr0_01.png}
\subcaption{learning rate $\alpha=0.01$}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{../hw1/part1/3_1_B5_lr0_03}
\subcaption{learning rate $\alpha=0.03$}
\end{subfigure}

\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.8\linewidth]{../hw1/part1/3_1_B5_lr0_1.png}
\subcaption{learning rate $\alpha=0.1$}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.8\textwidth]{../hw1/part1/3_1_B5_lr0_3}
\subcaption{learning rate $\alpha=0.3$}
\label{fig:converg_lr}
\end{subfigure}

\caption{Convergence of gradient descent for linear regression with multiple variables using different learning rate.}
\end{figure}

$\alpha = 0.1, 0.3$ and $N_{iteration} = 80$ are good trade off between accuracy and efficiency. By observing Fig.~\ref{fig:converg_lr} , one can easily find that small $\alpha$ (i.e. $\alpha= 0.01, 0.03$) leads to very slow convergence rate. $\alpha =0.1,0.3$ on the other hands, converges swiftly.

\subsection*{3.2.A1: Regularized linear regression cost function}
No figures for this question.

\subsection*{3.2.A2: Gradient of the regularized linear regression cost function}
Figure~\ref{fig:3_2_A2} shows the fitted curve of the linear model.
\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{../hw1/part2/fig3_2_A2_Linear.png}
\caption{The fitted curve of the linear model.}
\label{fig:3_2_A2}
\end{figure}

\subsection*{3.2.A3: Learning curves}
Figure~\ref{fig:3_2_A3} shows the learning curve of the linear model.
\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{../hw1/part2/fig3_2_A3_Learning_curve_labdta_0.png}
\caption{Learning curve of the linear model.}
\label{fig:3_2_A3}
\end{figure}

\subsection*{3.2.A4: Adjusting the regularization parameter}
Figure~\ref{fig:3_2_A4} plots the polynomial fit and learning curves for each  value of $\lambda$, from which we draw the following conclusions:


\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.7\linewidth]{../hw1/part2/fig3_2_A4_Learning_curve_lambda_0.png}
\subcaption{learning rate $\lambda = 0$}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{../hw1/part2/fig3_2_A4_polynomial_fit_lambda_0.png}
\subcaption{Polynomial fit $\lambda=0$}
\end{subfigure}

\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.7\linewidth]{../hw1/part2/fig3_2_A4_learning_curve_lambda_1.png}
\subcaption{learning rate $\lambda = 1$}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{../hw1/part2/fig3_2_A4_polynomial_fit_lambda_1.png}
\subcaption{Polynomial fit $\lambda=1$}
\end{subfigure}

\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.7\linewidth]{../hw1/part2/fig3_2_A4_learning_curve_lambda_10.png}
\subcaption{learning rate $\lambda = 10$}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{../hw1/part2/fig3_2_A4_polynomial_fit_lambda_10.png}
\subcaption{Polynomial fit $\lambda=10$}
\end{subfigure}

\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.7\linewidth]{../hw1/part2/fig3_2_A4_learning_curve_lambda_100.png}
\subcaption{learning rate $\lambda = 100$}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.7\textwidth]{../hw1/part2/fig3_2_A4_polynomial_fit_lambda_100.png}
\subcaption{Polynomial fit $\lambda=100$}
\end{subfigure}

\caption{Convergence of gradient descent for linear regression with multiple variables using different learning rate.}
\label{fig:3_2_A4}
\end{figure}

If $\lambda$ is too small, the effect of penalty term can be neglected so that the regulation will not be implemented effectively -- the training model is still troubled by high-variation/over-fitting issue (as can be seen in Fig....). When $\lambda$ is too large, the loss function is in fact dominated by the penalty term, which is a slightly similar to the biased issue that we have too strong assumptions on the model. Therefore, the training model turns to be under-fit.  Only when $\lambda$ takes appropriate value that the regulation can works effectively.


\subsection*{3.2.A5 Selecting $\lambda$ using a validation set}
Figure~\ref{fig:3_2_A5} shows the variation in trainng/validation error with respect of regulation parameter $\lambda$.  For the current model, $\lambda=1$ is approximately a good choice for the training. We list several reasons to justify our choice:
\begin{itemize}
	\item The difference between training/validation error is too large when $\lambda$ is significantly smaller than 1, implying an over-fitting issue -- which is true because the penalty term characterized by $\lambda$ is too small to regulate the effect of excessive features.
	\item The difference between training/validation error becomes relatively small, but the absolute training/validation error becomes too large when $\lambda$ gets larger than $1$. This indicates that the penalty term dominates the loss function and we actually are making a strong assumption of the target function (of similar form as that of penalty term), i.e., too much bias / not enough variation. 
	\item $\lambda=1$ makes a good balance to avoid either overfitting or underfitting, i.e. the validation/training error is small ( the prediction performance is acceptable); the difference between validation/training error is small (the prediction model is reliable).
\end{itemize}
\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{../hw1/part2/fig3_2_A5.png}
\caption{Variation in training/validation error with $\lambda$}
\label{fig:3_2_A5}
\end{figure}


\subsection*{3.2.A6 Computing test set error}
Error when choosing best lamdba  1.0  is  30987.4826556 (USD).

\subsection*{3.2.A7 Plotting learning curves with randomly selected examples}
Figure~\ref{fig:3_2_A7} plot the averaged learning curve for $\lambda = 1$.

\begin{figure}[h]
\centering
\includegraphics[width=.6\textwidth]{../hw1/part2/fig3_2_A7.png}
\caption{Averaged learning curve for $\lambda = 1$}
\label{fig:3_2_A7}
\end{figure}

\
\part*{Bonus question:}
Please see  \textbf{bostonexp.ipynb} for our detailed solution.
\subsection*{ 4.1 Regularized Linear Regression}
Figure~\ref{fig:4_1}  plots the variation in training/testing error with $\lambda$ for regularized linear model.
\begin{itemize}
\item Best $\lambda=10$:
\item training error: 11.605155
\item test error: 12.6566689644
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{../hw1/part2/fig4_linear_lambda_10.png}
\caption{Variation in training/testing error with $\lambda$ for regularized linear model}
\label{fig:4_1}
\end{figure}



\subsection*{ 4.2 Selecting $\lambda$ with quadratic features}
Figure~\ref{fig:4_2} plots the variation in training/testing error with $\lambda$ for regularized linear model with quadratic features.
\begin{itemize}
\item Best $\lambda=0.3$:
\item training error:  4.256794
\item test error: 4.82815820863
\end{itemize}

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{../hw1/part2/fig4_quadratic_lambda_03.png}
\caption{Variation in training/testing error with $\lambda$ for regularized linear model with quadratic features}
\label{fig:4_2}
\end{figure}




\subsection*{ 4.3 Selecting $\lambda$ with cubic features}
\begin{figure}[h]
\centering
\includegraphics[width=.5\textwidth]{../hw1/part2/fig4_cubic_lambda_3.png}
\caption{Variation in training/testing error with $\lambda$ for regularized linear model with quadratic features}
\label{fig:4_3}
\end{figure}

Figure~\ref{fig:4_3} plots the variation in training/testing error with $\lambda$ for regularized linear model with quadratic features. 
\begin{itemize}
\item Best $\lambda=3$:
\item training error:  3.645624
\item test error: 4.73217608015
\end{itemize}







\subsection*{ Summary}
In above sections, we trained linear regression model with linear/quadratic/cubic features to predict Boston house price. We can observe that all three models we built embodied the power of regulation in that appropriate value of $\lambda$ gives the "sweet region" --- and even though the number of features grow dramatically as we square and even cube the features for more variation, the regulation term in the loss function successfully constrains the model from overfitting. The fact that models with linear, quadratic, cubic features are successively one better than the other shows that: given more features (variations) and proper control of overfitting, the more delicate model has better performance in prediction.












\end{document}

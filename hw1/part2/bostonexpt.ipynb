{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building regularized models for Boston data set\n",
    "Perform a bias variance analysis of the Boston housing data set with the thirteen predictors, following the steps on the simple data set above. Use sklearn's  built-in functions to split the data into training, validation and test sets. What is the lowest achievable error on the test set with $\\lambda = 0$? Select the best value for $\\lambda$ and report the test set error with the best $\\lambda$. Use the technique of adding features to extend each column of the Boston data set with powers of the values in the column. Repeat the bias-variance analysis with quadratic and cubic features. What is the test set error with quadratic features with the best $\\lambda$ chosen with the validation set? What is the test set error with cubic features with the best $\\lambda$ chosen with the validation set? Put your analysis code in a separate Python script or notebook called bostonexpt.py or bostonexpt.ipynb. Present your results analytically with plots to support your findings. Discuss the impact of regularization for building good models for the Boston housing data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "will start by loading and displaying some values from the full Boston housing dataset with thirteen features of census tracts that are believed to be predictive of the median home price in the tract (see **housing.names.txt** for a full description of these features). By looking at the values, you will note that the values of some of the features are  about 1000 times the values of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Reading data ...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils\n",
    "import plot_utils\n",
    "from reg_linear_regressor_multi import RegularizedLinearReg_SquaredLoss\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print 'Reading data ...'\n",
    "bdata = load_boston()\n",
    "df = pd.DataFrame(data = bdata.data, columns = bdata.feature_names)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide data into training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size  (379, 13)\n",
      "Validation set size  (95, 13)\n",
      "Test set size  (32, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test_val, y_train, y_test_val = train_test_split(df, bdata.target)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test_val, y_test_val)\n",
    "\n",
    "print 'Training set size ', X_train.shape\n",
    "print 'Validation set size ', X_val.shape\n",
    "print 'Test set size ', X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression cost function and gradient (vectorized)\n",
    "Regularized linear regression has the following cost function:\n",
    "$$J(\\theta) = \\frac{1}{2m} \\left( \\sum_{i=1}^{m} {( y^{(i)} - h_\\theta(x^{(i)})}^2\\right) + \\frac{\\lambda}{2m}\\left( \\sum_{j=1}^{n} {\\theta_j}^2 \\right) $$\n",
    "where $\\lambda$ is a regularization parameter which controls the degree of regularization\n",
    "(thus, help preventing overfitting). The regularization term puts\n",
    "a penalty on the overall cost $J(\\theta)$. As the magnitudes of the model parameters $\\theta_j$ increase, the penalty increases as well. Note that you should not regularize the $\\theta_0$ term. You should now complete the code for the method loss in the class Reg_LinearRegression_SquaredLoss in the file reg_linear_regressor_multi.py to calculate $J(\\theta)$. Vectorize your code and avoid writing for loops.\n",
    "\n",
    "Correspondingly, the partial derivative of the regularized linear regression  cost function\n",
    "with respect to  $\\theta_j$ is defined as:\n",
    "\\begin{eqnarray*}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_0} & = & \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)}){x_j}^{(i)} \\\\\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} & = & \\left(\\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)} ){x_j}^{(i)}\\right) + \\frac{\\lambda}{m}{\\theta_j} \\; \\; \\; \\mbox{ for }j \\geq 1\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This training function uses scipy's fmin_bfgs to optimize the cost function.\n",
    "Here we have set the regularization parameter $\\lambda$ to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XX_train = utils.feature_normalize(X_)\n",
    "\n",
    "XX = np.vstack([np.ones((X.shape[0],)),X]).T\n",
    "\n",
    "#  Train linear regression with lambda = 0\n",
    "\n",
    "reglinear_reg1 = RegularizedLinearReg_SquaredLoss()\n",
    "theta_opt0 = reglinear_reg1.train(XX,y,reg=0.0,num_iters=1000)\n",
    "print 'Theta at lambda = 0 is ', theta_opt0\n",
    "\n",
    "# plot fit over data and show it (or save it in fig7.pdf)\n",
    "plot_utils.plot_data(X,y,'Change in water level (x)','Water flowing out of the dam (y)')\n",
    "plt.plot(X,np.dot(XX,theta_opt0),'g-',linewidth=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
